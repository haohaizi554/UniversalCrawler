# app/spiders/missav_spider.py

import re
import time
import urllib.parse
from collections import defaultdict
from playwright.sync_api import sync_playwright
from app.spiders.base_spider import BaseSpider

class MissAVSpider(BaseSpider):

    def run(self):
        try:
            # é…ç½®è§£æ (ä¿æŒä¸å˜)
            proxy_server = None
            if self.config.get('proxy'):
                proxy_server = self.config['proxy']
                self.log(f"ğŸŒ ä½¿ç”¨ä»£ç†: {proxy_server}")
            enable_individual = self.config.get('individual_only', False)
            priority_text = self.config.get('priority', "ä¸­æ–‡å­—å¹•ä¼˜å…ˆ")
            priority_map = {
                "ä¸­æ–‡å­—å¹•ä¼˜å…ˆ": ["ä¸­æ–‡å­—å¹•", "æ— ç æµå‡º", "è‹±æ–‡å­—å¹•", "æ™®é€šç‰ˆ"],
                "æ— ç æµå‡ºä¼˜å…ˆ": ["æ— ç æµå‡º", "ä¸­æ–‡å­—å¹•", "è‹±æ–‡å­—å¹•", "æ™®é€šç‰ˆ"]
            }
            self.priority_list = priority_map.get(priority_text, priority_map["ä¸­æ–‡å­—å¹•ä¼˜å…ˆ"])
            self.log(f"âš™ï¸ åå¥½è®¾ç½®: å•ä½“={enable_individual}, ä¼˜å…ˆçº§={self.priority_list}")
            my_ua = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
            # è·¯ç”±è§£æ (ä¿æŒä¸å˜)
            target_url = ""
            is_single_video_mode = False
            is_search_mode = False
            raw_input = self.keyword.strip()
            if "http" in raw_input:
                parsed = urllib.parse.urlparse(raw_input)
                path_parts = parsed.path.strip('/').split('/')
                keywords_list = ['search', 'actresses', 'tags', 'series', 'makers', 'directors', 'labels']
                if not any(k in parsed.path for k in keywords_list) and re.search(r'\w+-\d+', path_parts[-1]):
                    is_single_video_mode = True
                    target_url = raw_input
                    self.log("ğŸ”— è¯†åˆ«ä¸ºå•ä½“è§†é¢‘é“¾æ¥")
                else:
                    target_url = raw_input
                    self.log("ğŸ”— è¯†åˆ«ä¸ºåˆ—è¡¨/åˆ†ç±»é“¾æ¥")
            else:
                is_search_mode = True
                encoded_kw = urllib.parse.quote(raw_input)
                target_url = f"https://missav.ai/cn/search/{encoded_kw}"
                self.log(f"ğŸ” æ„é€ æœç´¢é“¾æ¥: {target_url}")
            if not is_single_video_mode:
                target_url = self._inject_url_params(target_url, enable_individual)
                self.log(f"ğŸ”§ ä¿®æ­£å URL: {target_url}")
            if not self.is_running: return
            # å¯åŠ¨æµè§ˆå™¨
            with sync_playwright() as p:
                browser = p.chromium.launch(
                    headless=False,
                    proxy={"server": proxy_server} if proxy_server else None,
                    args=['--disable-blink-features=AutomationControlled']
                )
                context = browser.new_context(user_agent=my_ua)
                page = context.new_page()
                self.log("ğŸš€ æ­£åœ¨è®¿é—®é¡µé¢...")
                page.goto(target_url, timeout=60000)
                if "Just a moment" in page.title():
                    self.log("ğŸ›¡ï¸ æ£€æµ‹åˆ° Cloudflareï¼Œç­‰å¾…é€šè¿‡...")
                    page.wait_for_timeout(5000)
                    page.wait_for_load_state("domcontentloaded")
                # å¤´åƒè·³è½¬
                if is_search_mode and self.is_running:
                    try:
                        time.sleep(2)
                        links = page.query_selector_all('a[href*="/actresses/"]')
                        valid_actress_link = None
                        for link in links:
                            href = link.get_attribute("href")
                            if href and "ranking" not in href and "search" not in href:
                                if link.is_visible():
                                    valid_actress_link = link
                                    break
                        if valid_actress_link:
                            href = valid_actress_link.get_attribute("href")
                            self.log(f"âœ¨ å‘ç°æ¼”å‘˜ä¸»é¡µï¼Œè‡ªåŠ¨è·³è½¬: {href}")
                            valid_actress_link.click()
                            page.wait_for_load_state('domcontentloaded')
                            current_url = page.url
                            new_url = self._inject_url_params(current_url, enable_individual)
                            if new_url != current_url:
                                page.goto(new_url)
                    except:
                        pass
                if not self.is_running:
                    browser.close()
                    return
                # æ•°æ®é‡‡é›†
                final_tasks = []
                if is_single_video_mode:
                    title = page.title().replace('| MissAV', '').strip()
                    final_tasks.append({'title': title, 'url': page.url})
                else:
                    scraped_data = {}
                    verified_chinese = set()
                    base_url = page.url
                    # --- Pass 1: ä¸»éå† ---
                    self.log("ğŸ“œ å¼€å§‹ç¬¬ä¸€éæ‰«æ (è·å–æ‰€æœ‰è§†é¢‘)...")
                    self._scan_pages(page, scraped_data, is_chinese_pass=False)

                    if not self.is_running:
                        self.log("â¸ï¸ æ‰«æè¢«ä¸­æ–­ï¼Œè·³è¿‡ä¸­æ–‡æ ¡éªŒï¼Œå‡†å¤‡ç”Ÿæˆæ¸…å•...")
                    if not scraped_data:
                        self.log("âŒ æœªæ‰¾åˆ°ä»»ä½•è§†é¢‘")
                        browser.close()
                        return
                    # --- Pass 2: ä¸­æ–‡æ ¡éªŒ (ä»…å½“æœªåœæ­¢æ—¶æ‰§è¡Œ) ---
                    # åªæœ‰å½“è¿˜åœ¨è¿è¡Œæ—¶ï¼Œæ‰å»æ‰«ç¬¬äºŒé
                    if self.is_running:
                        self.log("ğŸ‡¨ğŸ‡³ å¼€å§‹ç¬¬äºŒéæ‰«æ (æ ¡éªŒä¸­æ–‡å­—å¹•)...")
                        chinese_url = self._add_chinese_filter(base_url)

                        if chinese_url != base_url:
                            self.log(f"   è·³è½¬æ ¡éªŒ: {chinese_url}")
                            try:
                                chinese_url_no_page = re.sub(r'[?&]page=\d+', '', chinese_url)
                                page.goto(chinese_url_no_page, timeout=60000)

                                chinese_data = {}
                                self._scan_pages(page, chinese_data, is_chinese_pass=True)
                                verified_chinese = set(chinese_data.keys())
                                scraped_data.update(chinese_data)
                            except Exception as e:
                                self.log(f"   âš ï¸ ä¸­æ–‡æ ¡éªŒå¼‚å¸¸: {e}")

                    if not self.is_running and scraped_data:
                        self.is_running = True
                    # --- æ™ºèƒ½åˆ†ç»„æ‰“åˆ† ---
                    self.log(f"ğŸ§  æ™ºèƒ½ç­›é€‰ä¸­ (å…± {len(scraped_data)} ä¸ªå€™é€‰)...")
                    grouped = defaultdict(list)
                    code_pattern = re.compile(r'/cn/.*?([a-zA-Z]+-\d+)')

                    for url, title in scraped_data.items():
                        code = None
                        match = code_pattern.search(url)
                        if match: code = match.group(1).upper()

                        if code:
                            grouped[code].append((url, title))
                        else:
                            grouped[url].append((url, title))

                    for code, items in grouped.items():
                        sorted_items = sorted(
                            items,
                            key=lambda x: self._calculate_score(x[0], x[1], verified_chinese),
                            reverse=True
                        )
                        best_url, best_title = sorted_items[0]
                        final_title = self._generate_display_title(best_url, best_title, verified_chinese)
                        final_tasks.append({'title': final_title, 'url': best_url})

                # ================= 4. ç”¨æˆ·äº¤äº’ =================
                if not final_tasks:
                    self.log("âŒ ç­›é€‰åæ— æœ‰æ•ˆç»“æœ")
                    browser.close()
                    return
                self.log(f"ğŸ”” æ‰«æå®Œæˆï¼Œå…± {len(final_tasks)} ä¸ªæœ€ä½³ç‰ˆæœ¬")

                # ç¡®ä¿å¤æ´»ï¼Œå¦åˆ™å¼¹çª—é€»è¾‘ä¼šç«‹å³é€€å‡º
                self.is_running = True
                # å¼¹çª—é€‰æ‹©
                selected_indices = self.ask_user_selection(final_tasks)
                # å¦‚æœæ­¤æ—¶è¿”å› Noneï¼Œè¯´æ˜ç”¨æˆ·åœ¨å¼¹çª—é‡Œç‚¹äº†â€œå–æ¶ˆâ€
                if not selected_indices:
                    self.log("âŒ ç”¨æˆ·å–æ¶ˆä¸‹è½½")
                    browser.close()
                    return
                self.log(f"âœ… é€‰ä¸­ {len(selected_indices)} ä¸ªï¼Œå¼€å§‹å—…æ¢ m3u8...")

                # ================= 5. è¯¦æƒ…é¡µå—…æ¢ (playlist.m3u8) =================
                success_count = 0
                for i, idx in enumerate(selected_indices):
                    if not self.is_running: break
                    task = final_tasks[idx]
                    target_page_url = task['url']
                    title = task['title']
                    self.log(f"ğŸ•µï¸ [{i + 1}/{len(selected_indices)}] å—…æ¢: {title[:15]}...")
                    m3u8_url = None
                    def handle_request(req):
                        nonlocal m3u8_url
                        if "playlist.m3u8" in req.url:
                            m3u8_url = req.url
                    def on_popup(popup):
                        if popup != page:
                            try:
                                popup.close()
                            except:
                                pass
                    context.on("page", on_popup)
                    page.on("request", handle_request)
                    try:
                        page.goto(target_page_url, timeout=60000)
                        if "Just a moment" in page.title():
                            time.sleep(10)
                        try:
                            page.wait_for_selector(".plyr", timeout=5000)
                            page.mouse.click(400, 300)
                            time.sleep(2)
                            if not m3u8_url: page.mouse.click(400, 300)
                        except:
                            pass
                        for _ in range(15):
                            if m3u8_url or not self.is_running: break
                            time.sleep(1)
                        if not self.is_running: break
                        if m3u8_url:
                            self.log("   âœ¨ å—…æ¢æˆåŠŸ")
                            self.emit_video(
                                url=m3u8_url,
                                title=title,
                                source="missav",
                                meta={
                                    "referer": target_page_url,
                                    "ua": my_ua,
                                    "proxy": proxy_server
                                }
                            )
                            success_count += 1
                        else:
                            self.log("   âš ï¸ å—…æ¢è¶…æ—¶ (æœªæ‰¾åˆ° playlist.m3u8)")
                    except Exception as e:
                        self.log(f"   âŒ é¡µé¢åŠ è½½é”™è¯¯: {e}")
                    page.remove_listener("request", handle_request)
                    context.remove_listener("page", on_popup)
                    time.sleep(1)
                if self.is_running:
                    self.log(f"ğŸ‰ ä»»åŠ¡ç»“æŸï¼ŒæˆåŠŸæäº¤: {success_count}")
                else:
                    self.log("ğŸ›‘ ä»»åŠ¡å¼ºåˆ¶ä¸­æ­¢")
                browser.close()
        except Exception as e:
            self.log(f"ğŸ’¥ çˆ¬è™«é”™è¯¯: {e}")
        finally:
            self.sig_finished.emit()

    def _inject_url_params(self, url, individual_only):
        parsed = urllib.parse.urlparse(url)
        qs = urllib.parse.parse_qs(parsed.query)
        if individual_only:
            filters = qs.get('filters', [''])[0]
            parts = filters.split(',') if filters else []
            if 'individual' not in parts:
                parts.append('individual')
                new_filters = ",".join([p for p in parts if p])
                qs['filters'] = [new_filters]
        new_query = urllib.parse.urlencode(qs, doseq=True)
        return urllib.parse.urlunparse(parsed._replace(query=new_query))

    def _add_chinese_filter(self, url):
        parsed = urllib.parse.urlparse(url)
        qs = urllib.parse.parse_qs(parsed.query)
        filters = qs.get('filters', [''])[0]
        parts = filters.split(',') if filters else []
        if 'chinese-subtitle' not in parts:
            parts.append('chinese-subtitle')
            qs['filters'] = [",".join([p for p in parts if p])]
        new_query = urllib.parse.urlencode(qs, doseq=True)
        return urllib.parse.urlunparse(parsed._replace(query=new_query))

    def _scan_pages(self, page, data_dict, is_chinese_pass=False):
        current_page = 1
        max_pages = 100
        base_url = page.url
        while self.is_running and current_page <= max_pages:
            self.log(f"   ğŸ“„ æ‰«æç¬¬ {current_page} é¡µ...")
            try:
                page.wait_for_selector("div.grid", timeout=10000)
                items = page.evaluate('''() => {
                    return Array.from(document.querySelectorAll('div.grid a')).map(a => {
                        const img = a.querySelector('img');
                        const title = img ? img.getAttribute('alt') : a.textContent.trim();
                        return {
                            url: a.href,
                            title: title || "" 
                        };
                    });
                }''')

                code_pattern = re.compile(r'/cn/.*[a-zA-Z]+-\d+')
                new_count = 0
                for item in items:
                    url = item['url']
                    title = item['title']
                    if "/cn/" in url and code_pattern.search(url):
                        if not any(x in url for x in ['contact', 'dmca']):
                            if url not in data_dict:
                                data_dict[url] = title
                                new_count += 1

                if not page.query_selector("a[rel='next']"):
                    break
                current_page += 1
                if "page=" in base_url:
                    next_url = re.sub(r'page=\d+', f'page={current_page}', base_url)
                else:
                    sep = "&" if "?" in base_url else "?"
                    next_url = f"{base_url}{sep}page={current_page}"
                try:
                    page.goto(next_url, timeout=60000)
                except:
                    break
            except Exception as e:
                self.log(f"   âš ï¸ é¡µé¢æ‰«æå¼‚å¸¸: {e}")
                break

    def _calculate_score(self, url, title, verified_chinese):
        url_lower = url.lower()
        title_lower = title.lower()
        is_uncensored = "uncensored" in url_lower or "leak" in url_lower or "æ— ç " in title_lower
        is_english = "english" in url_lower or "è‹±æ–‡å­—å¹•" in title_lower
        is_chinese = (url in verified_chinese) or ("chinese" in url_lower) or ("ä¸­æ–‡å­—å¹•" in title_lower)
        if is_uncensored: is_chinese = False
        feature_map = {
            "ä¸­æ–‡å­—å¹•": is_chinese,
            "è‹±æ–‡å­—å¹•": is_english,
            "æ— ç æµå‡º": is_uncensored,
            "æ™®é€š": (not is_chinese and not is_english and not is_uncensored)
        }
        total = len(self.priority_list)
        for idx, name in enumerate(self.priority_list):
            score = (total - idx) * 20
            for key, satisfies in feature_map.items():
                if key in name and satisfies: return score
        return 0

    def _generate_display_title(self, url, title, verified_chinese):
        tags = []
        url_lower = url.lower()
        is_uncensored = "uncensored" in url_lower or "leak" in url_lower or "æ— ç " in title.lower()
        is_chinese = (url in verified_chinese) or ("chinese" in url_lower) or ("ä¸­æ–‡å­—å¹•" in title.lower())
        if is_uncensored:
            tags.append("[æ— ç ]")
            is_chinese = False
        if is_chinese: tags.append("[ä¸­å­—]")
        if "english" in url_lower: tags.append("[è‹±å­—]")
        tag_str = "".join(tags)
        return f"{tag_str} {title}" if tag_str else title